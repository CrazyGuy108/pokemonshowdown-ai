# Training Algorithm
This folder describes the neural network and the algorithm (based on [this](https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb) example) used for training it.

## Neural Network Architecture
The neural network takes in an [encoded](/src/ai/encodeBattleState.ts) [BattleState](/src/battle/state/BattleState.ts) vector and outputs a value associated with each possible [Choice](/src/battle/agent/Choice.ts) that can be made.

## Algorithm
1. Load the neural network to be trained, or [create](model.ts) one if it doesn't exist.
2. Allocate an [experience replay buffer](Memory.ts).
    * [Experiences](Experience.ts) are tuples that describe a state transition.
    * When the buffer is full, adding to it again will discard the oldest Experience tuple
3. Play a couple [games](battle.ts) completely randomly (against itself) to populate the buffer.
4. Play some games semi-randomly to train.
    * The semi-random part uses an [epsilon-greedy](https://en.wikipedia.org/wiki/Multi-armed_bandit) strategy, where the probability of making a random choice is high at the beginning of training and low towards the end.
    * For each decision that was fully handled, make a learning step with the network using a randomly sampled mini-batch from the experience replay buffer.
        * This uses the Bellman equation described [here](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/).
        * During training, an additional input is added to the neural network which is used to select which Choice value to output, which allows fitting the network to a single target action value.
        * If this didn't happen, the target action value would have to be inserted into a vector generated by feedforwarding the network, which is incorrect since it suggests that all other action values are correct and can interfere with other examples in a training batch.
